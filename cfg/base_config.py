from contexts.terrain_utils import *
import numpy as np
import torch

DEVICE = torch.device("cuda:0")

class BaseVehicleCfg:
    device = DEVICE
    
    class context:
        """
        A context is drawn at the start of every MDP episode. It determines the transition, reward, 
        and boundary (termination) function of the MDP. It is defined by a set of variables with a range.
        """
        # privileged_context is the context that is only available during training
        class dof:
            """
            Context for degree of freedom (actuator)
            """
            friction = [0.01, 1.0]
            drive_mode = 2  # 0: None, 1: Position, 2: Velocity, 3: Torque
            damping = [100, 500]
            default_friction = 0.0
            default_damping = 100
            dim = 2 # damping and friction

        class rigid_body_base:
            """
            Context for base rigid body property 
            """
            base_mass_displacement = [-10, 10]
            base_inertia_displacement_x = [-0.2, 0.5]
            base_inertia_displacement_y = [-.2, 0.5]
            base_inertia_displacement_z = [-.2, 0.5]
            inertia_lower_bound = 0.01 # the minimum inertia value
            dim = 4 # mass and three-dim inertia 
        
        class ground:
            """
            Context for the ground surface property
            """
            default_friction = 1.0
            default_restitution = 0.0
            # We follow the standard quadruped training setups
            friction = [0.5, 5.0]
            restitution = [0.0, 0.5]
        
        class action_scale:
            """
            Context for scaling the commanded action magnitude.
            """
            action_scale_lower = torch.tensor([0.6, 1.6], device=DEVICE)
            action_scale_upper = torch.tensor([1.0, 2.0], device=DEVICE)
            dim = 2

        class environment:
            """
            The most complex context which determines the environment properties.
            """
            sampler = 'isaac'
            curriculum = 'uniform'
            ddpm_model_path = 'contexts/denoising-diffusion-pytorch/model-190.pt'
            edm_model_path = 'contexts/edm/canopy-training-runs/00003-fusion-uncond-ddpmpp-edm-gpus1-batch24-fp32/network-snapshot-015004.pkl'
            train_path = 'assets/elevation/train/'
            # number of environments per row
            num_rows = 10
            num_rows_distill = 2
            # number of environments per column 
            num_cols = 10
            num_cols_distill = 2
            # number of rows and columns per environment
            num_rows_per_env = 128
            num_cols_per_env = 128
            # resolution of the x- and y-axes for each environment map 
            x_res = 0.1 # meters
            y_res = 0.1 # meters
            # scale for the height 
            z_scale = 0.1 # meters
            isaac_terrain_type = [
                                random_uniform_terrain,
                                sloped_terrain, 
                                #   pyramid_sloped_terrain, 
                                discrete_obstacles_terrain, 
                                wave_terrain, 
                                #   stairs_terrain, 
                                #   pyramid_stairs_terrain, 
                                # stepping_stones_terrain
                                ]
            isaac_terrain_args = [
                {'min_height': -0.45, 'max_height': 0.45, 'step': 0.06, 'downsampled_scale': 0.5},
                {'slope': np.sqrt(3) / 2},    ## [0.05, np.sqrt(3) / 2]
                {'max_height': 10, "min_size": 0.1, 'max_size': 2.0, 'num_rects': 20},   ## num_rects [1, 20]
                {'num_waves': 20, 'amplitude': 4},   ## [1, 20]   [0.1, 1]
                # {'step_width': 0.75, 'step_height': 0.1},
                # {'step_width': 0.75, 'step_height': -0.1},
                # {'stone_size': 1.0, 'stone_distance': 1.5, "max_height": 4, "platform_size": 1.0, 'depth': -1.0}
            ]
            
    class state_space:
        """
        The state space defines the boundary (minimum and maximum value) of the state
        variables. During training, the robot is generated by sampling uniformly from the state space range.
        """
        # quaternion (x, y, z, w)
        # linear velocity (x, y, z)
        # angular velocity (x, y, z)
        # goal vector (x', y')
        dim = 12
        use_globalmap = False
        use_localmap = False
        use_noise = True

        class angle:
            dim = 4 
            # roll, pitch, yaw
            angle_min = torch.tensor([0.0] * 3, device=DEVICE)
            # maximum angle for three axes
            angle_max = torch.tensor([np.deg2rad(15), np.deg2rad(15), 2 * np.pi], device=DEVICE)

            class noise:
                # noise added to the angle
                angle_noise_std = torch.tensor([np.deg2rad(5), np.deg2rad(5), np.deg2rad(5)], device=DEVICE)

        class lin_velocity:
            dim = 3
            randomize = True 
            # linear velocity bound, m/s
            min_lin_vel = torch.tensor([-2.0, -0.2, -0.2], device=DEVICE) # we constrain the linear velocity in y and z directions
            max_lin_vel = torch.tensor([2.0, 0.2, 0.2], device=DEVICE)

            class noise:
                # noise added to the linear velocity
                lin_vel_noise_std = torch.tensor([0.1, 0.1, 0.1], device=DEVICE)
        
        class ang_velocity:
            dim = 3
            randomize = True
            # angular velocity bound, rad/s
            min_ang_vel = torch.tensor([-np.pi/20., -np.pi/20., -1.4], device=DEVICE)
            max_ang_vel = torch.tensor([np.pi/20., np.pi/20., 1.4], device=DEVICE)

            class noise:
                # noise added to the angular velocity
                ang_vel_noise_std = torch.tensor([0.1, 0.1, 0.1], device=DEVICE)
        
        class lin_acc:
            lin_acc_mag_limit = 2.0 # m/s^2

        class ang_acc:
            ang_acc_mag_limit = 1.4 # rad/s^2

        class goal:
            """
            Relative distance from the robot's current position to the goal position.
            """
            dim = 2
            min_goal_vec = torch.tensor([0.0, 0.0], device=DEVICE)
            max_goal_vec = torch.tensor([12.0, 12.0], device=DEVICE)
            # desired velocity at the goal
            desired_goal_lin_vel = 0.0
            # the robot is at the goal if it has very small linear and angular velocities
            goal_lin_vel_eps = 1e-1
            goal_ang_vel_eps = 1e-1

        class desired_lin_velocity:
            """
            The desired velocity of the robot lin_vel[:, 0] - self.cfg.state_space.goal.desired_goal_lin_vel)t.
            """
            min_desired_lin_velocity = -2.0
            max_desired_lin_velocity = 2.0

    class action_space:
        """
        Linear and angular velocities
        """
        # action space limit 
        action_lower = torch.tensor([-2.0, -1.4], device=DEVICE) 
        action_upper = torch.tensor([2.0, 1.4], device=DEVICE) 
        dim = 2
        

    class reward:
        # scales for each reward
        reward_scales = {
            'goal_reward': 5.0,
            # 'forward_vel_reward': 1e-3, # a reward for moving forward
            'orientation_reward': 1e-2,
            #'action_mag_reward': 1e-3,
            # 'lin_vel_reward': 1e-2,  # try if this will affect
            # 'ang_vel_reward': 1e-2,  # try if this will affect
            # 'action_smoothness_reward': 1e-2,
            # 'action_limits_reward': 0.0, 
            # 'backward_vel_reward': 0.0,
            # 'velocity_limit_reward': 1e-2,
            'collision_reward': 1e-2,
            # 'wheel_air_reward': 1e-3,   # try if this will affect
            # 'orientation_limit_reward': 0.0, 
            'jerk_reward': 1e-4,
            'goal_orientation_reward': 1e-3,
            'acceleration_limit_reward': 1e-2
        }

    class boundary:
        """
        Each state variable has a range, and exceeding the range will result in a boundary condition.
        """
        # determine whether to use the corresponding boundary conditions
        # the order specifies the priority of the boundary conditions, the last is the most important, 
        # and its boundary value will override the previous ones
        enabled_boundaries = [ 
            'state_space_boundary',
            'goal_success_boundary',
            'goal_fail_boundary',
            'episode_boundary'
        ]

        # conditions that terminate the episode
        boundary_conditions = {
            'state_space_boundary': 1.0, # dummy boolean, as the state space boundary conditions are determined by the state_space class 
            'goal_success_boundary': 0.1, # < dist, terminate
            'goal_fail_boundary': 13.0, # > dist, terminate
            # 'lin_acc_boundary': 20.0, # m/s^2, terminate
            # 'ang_acc_boundary': 25.0, # rad/s^2, terminate
            'collision_boundary': 1, # dummy boolean 
            # episode length; 
            # time = dt * episode_length * action_repeat; 
            # default time = 0.016 * 500 * 1 = 8.0s 
            # approximate maximum length = 8.0 * v_max (2) = 16m 
            'episode_boundary': 500,
        }


    ### Isaac Gym related configs ###
    class env:
        # environment's maximum dimensions that we sample the robot's initial positions
        env_x_len = 5.0
        env_y_len = 5.0
        env_z_len = 10.0
        # number of environments (agents) for training
        num_agents_per_terrain = 100
        num_agents_per_terrain_distill = 100
        action_repeat = 4 
        max_episode_length = 1000
    
    class robot:
        base_width = 0.0 # meters
        wheel_radius = 0.0 # meters
        # the height of the robot, used to determine the robot's initial z position
        base_height = 0.0
        name = ''
        control_limit_lower = torch.tensor([-2.0, -4.0], device=DEVICE)
        control_limit_upper = torch.tensor([2.0, 4.0], device=DEVICE)

        class camera:
            use_camera = False 
            type = 'stereo' # Optional: depth, stereo, mono
            # fov (87, 58); vertical_fov = horizontal_fov * img_size[1] / img_size[0]
            horizontal_fov = 87
            # Same configuration as the Intel realsense D435i: https://www.intelrealsense.com/depth-camera-d435i/
            img_size = (640, 480)
            far_plane = 3.0
            near_plane = 0.05
        
    class asset:
        asset_root = 'assets'
        asset_file = ''
        asset_name = 'robot'
        # merge bodies connected by fixed joints. Specific fixed joints can be kept by adding " <... dont_collapse="true"> in urdf files
        collapse_fixed_joints = True
        fix_base_link = False  # fixe the base of the robot
        self_collisions = 0  # 1 to disable, 0 to enable...bitwise filter
        # replace collision cylinders with capsules, leads to faster/more stable simulation
        replace_cylinder_with_capsule = True
        flip_visual_attachments = True  # Some .obj meshes must be flipped from y-up to z-up
        default_dof_drive_mode = 2
        override_inertia = True

        density = 0.001
        angular_damping = 0.
        linear_damping = 0.
        max_angular_velocity = 1000.
        max_linear_velocity = 1000.
        thickness = 0.01
    
    class sim:
        """
        Physics engine parameters
        """
        substeps = 1
        gravity = [0., 0., -9.81]  # [m/s^2]
        up_axis = 1  # 0 is y, 1 is z
        use_gpu_pipeline = True
        dt = 0.005
        class physx:
            num_threads = 0
            solver_type = 1  # 0: pgs, 1: tgs
            num_position_iterations = 4
            
            contact_offset = 0.01  # [m]
            rest_offset = 0.0  # [m]
            bounce_threshold_velocity = 0.5  # 0.5 [m/s]
            max_depenetration_velocity = 1.0
            max_gpu_contact_pairs = 2 ** 23  # 2**24 -> needed for 8000 envs and more
            default_buffer_size_multiplier = 5
            contact_collection = 2  # 0: never, 1: last sub-step, 2: all sub-steps (default=2)

    ### RL algorithm related configs ###
    class rl:
        # number of calls to ppo.update(); the total number of samples is 
        # N = num_learning_steps * num_envs * num_transitions_per_env
        num_learning_steps = 10000
        log_eval_interval = 2
        class ppo:
            lr = 5e-4 #1e-3
            # number of times for iterating over the entire dataset 
            ppo_epoch = 4 
            # the number of samples for each training batch 
            mini_batch_size = 50000 
            clip_param = 0.2
            value_loss_coeff = 1.0
            entropy_coeff = 0.005 #1e-3
            clip_grad_norm = 1.0
        class net:
            """
            Network configurations
            """
            num_policy_network_layers = 3 
            num_value_network_layers = 3
            action_noise_std = torch.tensor([1.0, 1.5], device=DEVICE)
            policy_network_hidden_dim = 256
            value_network_hidden_dim = 500 
            # the latent dimension for encoding the privileged context
            latent_dim = 10
        
        class actor_critic_net:
            action_std = torch.tensor([1.0, 2.0], device=DEVICE) 
            num_hidden_layers = 3
            hidden_dim = 300

        class buffer:
            # the number of transitions to store for each environment before each training starts
            # the total number of transitions is num_envs * num_transitions
            num_transitions_per_env = 100